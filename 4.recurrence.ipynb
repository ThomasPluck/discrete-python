{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Discrete Optimization can similarly be extended to Recurrent Neural Networks - since we have the steps for a multi-layer discrete optimization - we can apply these to the exact formula of a binarized Elman network and begin learning on a task that is more typical of a language model.\n",
    "\n",
    "We begin with boilerplate methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducability\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Feedforward Elman Network\n",
    "def elman_forward(Wh,Uh,Wy,bh,by,x,h):\n",
    "    out = np.sign(x@Wh+h@Uh-bh)\n",
    "    return out, np.sign(out@Wy-by)\n",
    "\n",
    "def compute_fpfn(z,zhat):\n",
    "       # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    false_pos = false_pos * 2 - 1\n",
    "    false_neg = false_neg * 2 - 1\n",
    "\n",
    "    return false_pos, false_neg\n",
    "\n",
    "# Discover which columns/bias terms are to blame\n",
    "def blame_columns(z,zhat,b_blame,bias,param):\n",
    "\n",
    "    # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    # Increment bias blame for false positives (too big!)\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            b_blame[idx] += 1\n",
    "\n",
    "    # Decrement bias blame for false negatives (too small!)\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            b_blame[idx] -= 1\n",
    "\n",
    "    # If bias threshold is crossed, reset blame and increment/decrement bias\n",
    "    for idx, i in enumerate(np.abs(b_blame)>param):\n",
    "        if np.sign(b_blame[idx]) > 0 and i:\n",
    "            bias[idx] += 1\n",
    "        elif np.sign(b_blame[idx]) < 0 and i:\n",
    "            bias[idx] -= 1\n",
    "\n",
    "    return false_pos, false_neg, b_blame, bias\n",
    "\n",
    "def blame_weights(x,false_pos,false_neg,W_blame,Weight,param):\n",
    "\n",
    "    # Binarize inputs\n",
    "    x = x > 0\n",
    "\n",
    "    # If a weight is found to be blame for a false positive attribute blame\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_not(np.logical_xor(Weight[:,idx]>0,x))):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 1\n",
    "\n",
    "    # If a weight is found to be blame for a false negative attribute blame\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_xor(Weight[:,idx]>0,x)):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 1\n",
    "\n",
    "    # Find where weights exceed the blame threshold\n",
    "    rows,cols = np.where(W_blame >= param)\n",
    "\n",
    "    # Reset blame counter and flip corresponding weight\n",
    "    for i,j in zip(rows,cols):\n",
    "        W_blame[i,j] = 0\n",
    "        Weight[i,j] = Weight[i,j] * -1\n",
    "    \n",
    "    return W_blame, Weight\n",
    "\n",
    "# Majority vote inter-layer glue\n",
    "def majority_vote(z,W,fp,fn):\n",
    "\n",
    "    out = np.zeros(W.shape[0])\n",
    "\n",
    "    if not fp.any() and not fn.any():\n",
    "        return z\n",
    "\n",
    "    for idx, i in enumerate(fp):\n",
    "        if i:\n",
    "            out -= W[:,idx]\n",
    "    \n",
    "    for idx, i in enumerate(fn):\n",
    "        if i:\n",
    "            out += W[:,idx]\n",
    "            \n",
    "    return np.sign(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We'd like to load a more temporally sensitive dataset to prove that we are effectively training a recurrent neural net, of sorts. To do this, we'll be using the `reuters` dataset, which tokenizes 30945 words into 8982 articles into 45 topics - we will load and one hot encode this for training.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training of an Elman Network is a little more complex, since we must do \"Discrete Optimization Through Time\" - the concept itself isn't too difficult to realize since all we have to do is introduce a depth limit like typical RNNs. We'll consider the weight matrices and biases to be static (even though they really aren't) and hope that our tuning signal isn't too noisy for some sufficiently small depth of optimization.\n",
    "\n",
    "In order to explain Elman Network training, we should probably define an Elman Network. An Elman Network is defined by the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_t &= \\sigma_h(W_hx_t+U_h h_{t-1} +b_h)\\\\\n",
    "y_t &= \\sigma_y(W_y h_t+b_y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note: $W_h\\neq W_y$. So our training will work as usual for the $y_t$ step, utilizing the above boilerplate - blame attribution for the recursive step is little more tricky. In that case the various $x_t$ must be cached, and so to the labels (ie. ground truth) of the $h_t$, to blame the two weight matrices, we just pass these independently through the `blame_weights` function and derive appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 21:15:24.696412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:24.804695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:24.804971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:24.805853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 21:15:24.806680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:24.806925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:24.807116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:25.252683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:25.252992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:25.253207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 21:15:25.253404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4661 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-11-13 21:15:25.255353: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load MNIST data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mreuters\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m----> 6\u001b[0m x_train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mone_hot(x_train,depth\u001b[39m=\u001b[39;49m\u001b[39m30945\u001b[39;49m)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      7\u001b[0m x_test \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mone_hot(x_test,depth\u001b[39m=\u001b[39m\u001b[39m30945\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      8\u001b[0m y_train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mone_hot(y_train,depth\u001b[39m=\u001b[39m\u001b[39m45\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py:6404\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   6402\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6403\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6404\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[1;32m   6405\u001b[0m       _ctx, \u001b[39m\"\u001b[39m\u001b[39mOneHot\u001b[39m\u001b[39m\"\u001b[39m, name, indices, depth, on_value, off_value, \u001b[39m\"\u001b[39m\u001b[39maxis\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   6406\u001b[0m       axis)\n\u001b[1;32m   6407\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6408\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data()\n",
    "\n",
    "def binarize(input: np.array):\n",
    "    return (input < 0.5) * 2 - 1\n",
    "\n",
    "INPUT_SIZE = 30945 # Number of one-hot words in reuters dataset\n",
    "HIDDEN_SIZE = 2048 # Number of hidden units that we want\n",
    "OUTPUT_SIZE = 45 # How many output nodes should there be\n",
    "\n",
    "# Initialize Elman Network weights\n",
    "hidden_weight = binarize(np.random.uniform(0,1,(INPUT_SIZE,HIDDEN_SIZE))) # Denoted W_h in above formulae\n",
    "interi_weight = binarize(np.random.uniform(0,1,(HIDDEN_SIZE,HIDDEN_SIZE))) # Denoted U_h ...\n",
    "output_weight = binarize(np.random.uniform(0,1,(HIDDEN_SIZE,OUTPUT_SIZE))) # Denoted W_y ...\n",
    "\n",
    "# Initialize Elman Network biases\n",
    "hidden_bias = np.random.randint(0,INPUT_SIZE,HIDDEN_SIZE)\n",
    "output_bias = np.random.randint(0,HIDDEN_SIZE,OUTPUT_SIZE)\n",
    "\n",
    "# Initialize cache and cache parameters\n",
    "input_cache = []\n",
    "hidden_cache = [np.zeros(HIDDEN_SIZE) - 1]\n",
    "label_cache = []\n",
    "CACHE_DEPTH = 2\n",
    "\n",
    "# Blame counters\n",
    "hidden_weight_blame = np.zeros((INPUT_SIZE,HIDDEN_SIZE))\n",
    "interi_weight_blame = np.zeros((HIDDEN_SIZE,HIDDEN_SIZE))\n",
    "output_weight_blame = np.zeros((HIDDEN_SIZE,OUTPUT_SIZE))\n",
    "hidden_bias_blame = np.zeros(HIDDEN_SIZE)\n",
    "output_bias_blame = np.zeros(OUTPUT_SIZE)\n",
    "\n",
    "# Program counters\n",
    "epochs = 3\n",
    "counter = 0\n",
    "COUNT_RESET = 20\n",
    "acc = 0\n",
    "acc_count = 0\n",
    "\n",
    "# Iterate over epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(\"EPOCH \"+str(e+1))\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    input_cache = []\n",
    "    hidden_cache = [np.zeros(HIDDEN_SIZE) - 1]\n",
    "    label_cache = []\n",
    "    CACHE_DEPTH = 2\n",
    "\n",
    "    # Iterate over samples\n",
    "    for i in tqdm(indices):\n",
    "        \n",
    "        # Load training samples\n",
    "        x = binarize(tf.one_hot(x_train[i],INPUT_SIZE))\n",
    "        y = binarize(tf.one_hot(y_train[i],OUTPUT_SIZE))\n",
    "\n",
    "        \n",
    "        # Iterate over words in samples\n",
    "        for x1 in x:\n",
    "\n",
    "            # Cache input activations\n",
    "            input_cache.insert(0,x1)\n",
    "            input_cache = input_cache[:CACHE_DEPTH]\n",
    "            \n",
    "            # Forward step\n",
    "            prev_h = hidden_cache[0]\n",
    "            curr_h, yhat = elman_forward(hidden_weight,interi_weight,output_weight,\n",
    "                                         hidden_bias,output_bias,x1,prev_h)\n",
    "\n",
    "            # Cache new hidden activation\n",
    "            hidden_cache.insert(0,curr_h)\n",
    "            hidden_cache = hidden_cache[:CACHE_DEPTH]\n",
    "\n",
    "            # Begin backward pass of second Elman equation\n",
    "            fp, fn, output_bias_blame, output_bias = blame_columns(y,yhat,output_bias_blame,output_bias,16)\n",
    "            output_weight_blame, output_weight = blame_weights(curr_h, fp, fn, output_weight_blame, output_weight, 16)\n",
    "            h = majority_vote(y,output_weight,fp,fn)\n",
    "\n",
    "            # Backward pass of second Elman equation\n",
    "            fp, fn, hidden_bias_blame, hidden_bias = blame_columns(h,curr_h,hidden_bias_blame,hidden_bias,16)\n",
    "            hidden_weight_blame, hidden_weight = blame_weights(x,fp,fn,hidden_weight_blame,hidden_weight,16)\n",
    "            interi_weight_blame, interi_weight = blame_weights(prev_h,fp,fn,interi_weight_blame,interi_weight,16)\n",
    "\n",
    "            # Forgiveness counter\n",
    "            counter += 1 - np.sum(np.logical_and(yhat > 0, y > 0))\n",
    "            if counter >= COUNT_RESET:\n",
    "                hidden_weight_blame -= 1\n",
    "                hidden_weight_blame[hidden_weight_blame < 0] = 0\n",
    "                interi_weight_blame -= 1\n",
    "                interi_weight_blame[interi_weight_blame < 0] = 0\n",
    "                output_weight_blame -= 1\n",
    "                output_weight_blame[output_weight_blame < 0] = 0\n",
    "                hidden_bias_blame = np.sign(hidden_bias_blame) * (np.abs(hidden_bias_blame) - 1)\n",
    "                output_bias_blame = np.sign(output_bias_blame) * (np.abs(output_bias_blame) - 1)\n",
    "                counter = 0\n",
    "\n",
    "            # Accuracy metric counter\n",
    "            acc += np.sum(np.logical_and(yhat > 0, y > 0))\n",
    "            acc_count += 1\n",
    "        \n",
    "        print(\"Sample Accuracy: \",acc/acc_count)\n",
    "        acc, acc_count = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
