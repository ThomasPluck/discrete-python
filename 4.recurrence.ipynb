{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Discrete Optimization can similarly be extended to Recurrent Neural Networks - since we have the steps for a multi-layer discrete optimization - we can apply these to the exact formula of a binarized Elman network and begin learning on a task that is more typical of a language model.\n",
    "\n",
    "We begin with boilerplate methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducability\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Feedforward Elman Network\n",
    "def elman_forward(Wh,Uh,Wy,bh,by,x,h):\n",
    "    out = np.sign(x@Wh+h@Uh-bh)\n",
    "    out[out == 0] = 1\n",
    "    out2 = np.sign(out@Wy-by)\n",
    "    out[out == 0] = 1\n",
    "    return out, out2\n",
    "\n",
    "def compute_fpfn(z,zhat):\n",
    "       # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    false_pos = false_pos * 2 - 1\n",
    "    false_neg = false_neg * 2 - 1\n",
    "\n",
    "    return false_pos, false_neg\n",
    "\n",
    "# Discover which columns/bias terms are to blame\n",
    "def blame_columns(z,zhat,b_blame,bias,param,depth=0):\n",
    "\n",
    "    # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    # Increment bias blame for false positives (too big!)\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            b_blame[idx] += 2**-depth\n",
    "\n",
    "    # Decrement bias blame for false negatives (too small!)\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            b_blame[idx] -= 2**-depth\n",
    "\n",
    "    # If bias threshold is crossed, reset blame and increment/decrement bias\n",
    "    for idx, i in enumerate(np.abs(b_blame)>param):\n",
    "        if np.sign(b_blame[idx]) > 0 and i:\n",
    "            bias[idx] += 1\n",
    "        elif np.sign(b_blame[idx]) < 0 and i:\n",
    "            bias[idx] -= 1\n",
    "\n",
    "    return false_pos, false_neg, b_blame, bias\n",
    "\n",
    "def blame_weights(x,false_pos,false_neg,W_blame,Weight,param,depth=0):\n",
    "\n",
    "    # Binarize inputs\n",
    "    x = x > 0\n",
    "\n",
    "    # If a weight is found to be blame for a false positive attribute blame\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_not(np.logical_xor(Weight[:,idx]>0,x))):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 2**-depth\n",
    "\n",
    "    # If a weight is found to be blame for a false negative attribute blame\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_xor(Weight[:,idx]>0,x)):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 2**-depth\n",
    "\n",
    "    # Find where weights exceed the blame threshold\n",
    "    rows,cols = np.where(W_blame >= param)\n",
    "\n",
    "    # Reset blame counter and flip corresponding weight\n",
    "    for i,j in zip(rows,cols):\n",
    "        W_blame[i,j] = 0\n",
    "        Weight[i,j] = Weight[i,j] * -1\n",
    "    \n",
    "    return W_blame, Weight\n",
    "\n",
    "# Majority vote inter-layer glue\n",
    "def majority_vote(z,W,fp,fn):\n",
    "\n",
    "    out = np.zeros(W.shape[0])\n",
    "\n",
    "    if not fp.any() and not fn.any():\n",
    "        return z\n",
    "\n",
    "    for idx, i in enumerate(fp):\n",
    "        if i:\n",
    "            out -= W[:,idx]\n",
    "    \n",
    "    for idx, i in enumerate(fn):\n",
    "        if i:\n",
    "            out += W[:,idx]\n",
    "            \n",
    "    return np.sign(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We'd like to load a more temporally sensitive dataset to prove that we are effectively training a recurrent neural net, of sorts. To do this, we'll be using the `reuters` dataset, which tokenizes 30945 words into 8982 articles into 45 topics - we will load and one hot encode this for training.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training of an Elman Network is a little more complex, since we must do \"Discrete Optimization Through Time\" - the concept itself isn't too difficult to realize since all we have to do is introduce a depth limit like typical RNNs. We'll consider the weight matrices and biases to be static (even though they really aren't) and hope that our tuning signal isn't too noisy for some sufficiently small depth of optimization.\n",
    "\n",
    "In order to explain Elman Network training, we should probably define an Elman Network. An Elman Network is defined by the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_t &= \\sigma_h(W_hx_t+U_h h_{t-1} +b_h)\\\\\n",
    "y_t &= \\sigma_y(W_y h_t+b_y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note: $W_h\\neq W_y$. So our training will work as usual for the $y_t$ step, utilizing the above boilerplate - blame attribution for the recursive step is little more tricky. In that case the various $x_t$ must be cached, and so to the labels (ie. ground truth) of the $h_t$, to blame the two weight matrices, we just pass these independently through the `blame_weights` function and keep track of appropriate labels.\n",
    "\n",
    "To prevent exponential blow up (and admittedly in violation of the philosophy of discrete optimization), we'll also require allocated blame at $t$ time-steps back in history to be scaled down by $2^{-t}$ - we do this by adding an optional \"depth\" keyword argument to `blame_columns` and `blame_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8982 [02:38<395:49:34, 158.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Accuracy:  0.8279569892473119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8982 [07:55<627:55:17, 251.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Accuracy:  0.8172043010752689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/8982 [12:16<637:59:18, 255.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Accuracy:  0.8152173913043478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/8982 [13:17<446:38:10, 179.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Accuracy:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/8982 [14:15<337:54:36, 135.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data()\n",
    "\n",
    "def binarize(input: np.array):\n",
    "    return (input < 0.5) * 2 - 1\n",
    "\n",
    "INPUT_SIZE = 30945 # Number of one-hot words in reuters dataset\n",
    "HIDDEN_SIZE = 2048 # Number of hidden units that we want\n",
    "OUTPUT_SIZE = 45 # How many output nodes should there be\n",
    "\n",
    "# Initialize Elman Network weights\n",
    "hidden_weight = binarize(np.random.uniform(0,1,(INPUT_SIZE,HIDDEN_SIZE))) # Denoted W_h in above formulae\n",
    "interi_weight = binarize(np.random.uniform(0,1,(HIDDEN_SIZE,HIDDEN_SIZE))) # Denoted U_h ...\n",
    "output_weight = binarize(np.random.uniform(0,1,(HIDDEN_SIZE,OUTPUT_SIZE))) # Denoted W_y ...\n",
    "\n",
    "# Initialize Elman Network biases\n",
    "hidden_bias = np.random.randint(0,INPUT_SIZE,HIDDEN_SIZE)\n",
    "output_bias = np.random.randint(0,HIDDEN_SIZE,OUTPUT_SIZE)\n",
    "\n",
    "# Initialize cache and cache parameters\n",
    "input_cache = []\n",
    "hidden_cache = [np.zeros(HIDDEN_SIZE) - 1]\n",
    "label_cache = []\n",
    "CACHE_DEPTH = 2\n",
    "\n",
    "# Blame counters\n",
    "hidden_weight_blame = np.zeros((INPUT_SIZE,HIDDEN_SIZE))\n",
    "interi_weight_blame = np.zeros((HIDDEN_SIZE,HIDDEN_SIZE))\n",
    "output_weight_blame = np.zeros((HIDDEN_SIZE,OUTPUT_SIZE))\n",
    "hidden_bias_blame = np.zeros(HIDDEN_SIZE)\n",
    "output_bias_blame = np.zeros(OUTPUT_SIZE)\n",
    "\n",
    "# Program counters\n",
    "epochs = 3\n",
    "counter = 0\n",
    "COUNT_RESET = 20\n",
    "acc = 0\n",
    "acc_count = 0\n",
    "\n",
    "# Iterate over epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(\"EPOCH \"+str(e+1))\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    input_cache = []\n",
    "    hidden_cache = [np.zeros(HIDDEN_SIZE) - 1]\n",
    "    label_cache = []\n",
    "    CACHE_DEPTH = 2\n",
    "\n",
    "    # Iterate over samples\n",
    "    for i in tqdm(indices):\n",
    "        \n",
    "        # Load training samples\n",
    "        x = (tf.one_hot(x_train[i],INPUT_SIZE)) * 2 - 1\n",
    "        y = (tf.one_hot(y_train[i],OUTPUT_SIZE)) * 2 - 1\n",
    "\n",
    "        \n",
    "        # Iterate over words in samples\n",
    "        for x1 in x:\n",
    "\n",
    "            # Cache input activations\n",
    "            input_cache.insert(0,x1)\n",
    "            input_cache = input_cache[:CACHE_DEPTH]\n",
    "            \n",
    "            # Forward step\n",
    "            prev_h = hidden_cache[0]\n",
    "            curr_h, yhat = elman_forward(hidden_weight,interi_weight,output_weight,\n",
    "                                         hidden_bias,output_bias,x1,prev_h)\n",
    "\n",
    "            # Cache new hidden activation\n",
    "            hidden_cache.insert(0,curr_h)\n",
    "            hidden_cache = hidden_cache[:CACHE_DEPTH]\n",
    "\n",
    "            # Begin backward pass of second Elman equation\n",
    "            fp, fn, output_bias_blame, output_bias = blame_columns(y,yhat,output_bias_blame,output_bias,16)\n",
    "            output_weight_blame, output_weight = blame_weights(curr_h, fp, fn, output_weight_blame, output_weight, 16)\n",
    "            h = majority_vote(curr_h ,output_weight,fp,fn)\n",
    "\n",
    "            # Cache hidden ground truth\n",
    "            label_cache.insert(0,h)\n",
    "            label_cache = label_cache[:CACHE_DEPTH]\n",
    "\n",
    "            # Recursive backward pass of second Elman equation\n",
    "            for i in range(CACHE_DEPTH-1):\n",
    "                fp, fn, hidden_bias_blame, hidden_bias = blame_columns(label_cache[i],hidden_cache[i],hidden_bias_blame,hidden_bias,16,depth=i)\n",
    "                hidden_weight_blame, hidden_weight = blame_weights(input_cache[i],fp,fn,hidden_weight_blame,hidden_weight,16,depth=i)\n",
    "                interi_weight_blame, interi_weight = blame_weights(hidden_cache[i+1],fp,fn,interi_weight_blame,interi_weight,16,depth=i)\n",
    "\n",
    "            # Forgiveness counter\n",
    "            counter += 1 - np.sum(np.logical_and(yhat > 0, y > 0))\n",
    "            if counter >= COUNT_RESET:\n",
    "\n",
    "                hidden_weight_blame -= 1\n",
    "                hidden_weight_blame[hidden_weight_blame < 0] = 0\n",
    "                interi_weight_blame -= 1\n",
    "                interi_weight_blame[interi_weight_blame < 0] = 0\n",
    "                output_weight_blame -= 1\n",
    "                output_weight_blame[output_weight_blame < 0] = 0\n",
    "                \n",
    "                hidden_bias_blame = np.sign(hidden_bias_blame) * (np.abs(hidden_bias_blame) - 1)\n",
    "                output_bias_blame = np.sign(output_bias_blame) * (np.abs(output_bias_blame) - 1)\n",
    "                counter = 0\n",
    "\n",
    "            # Accuracy metric counter\n",
    "            acc += np.sum(np.logical_and(yhat > 0, y > 0))\n",
    "            acc_count += 1\n",
    "        \n",
    "        print(\"Sample Accuracy: \",acc/acc_count)\n",
    "        acc, acc_count = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
