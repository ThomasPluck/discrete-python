{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "The next logical goal in discrete optimization is extending it to multiple layers of perceptrons as in conventional deep learning. This is done by extending the concepts learnt in the perceptron chapter but by instead of attributing blame to weights, turning it on its head and attributing blame to the input instead.\n",
    "\n",
    "We do this by constructing an \"ideal input\" binarized vector $\\tilde{x}$ working backwards from the weights themselves and the desired output state of the network given by the ground truth $z$. We do this with a perculiar reinterpretation of perceptron equation:\n",
    "\n",
    "Suppose that the ground truth $z$ is positive in the $i^{\\text{th}}$ entry - we consider the weight matrix column $W_i$, if we were to maximize the dot product between $W_i$ and $\\tilde{x}$ we'd let $\\tilde{x} = W_i$, however, $\\hat{z}_i$ will be positive so long as $W_i\\cdot \\tilde{x} > b_i$ so, we can actually choose $\\tilde{x}$ to be at most $W_i\\cdot W_i - b_i$ in Hamming distance from $W_i$.\n",
    "\n",
    "We have to craft ideal inputs *for all* weight columns, so we end up with a family of ideal strings $\\tilde{x}_i$ which are $W_i$ if $z_i = +1$ and $-W_i$ if $z_i = -1$, and their bounding Hamming distances $r_i$ which are $W_i\\cdot W_i - b_i$ if $z_i=+1$ and $b_i$ if $z_i=-1$. This family defines a set of balls $B(\\tilde{x}_i,r_i)$ in Hamming space, which may or may not intersect.\n",
    "\n",
    "For the sake of argument, we then assume that the largest intersection (ie. the intersection of the most balls) contains the \"ideal input\" which we can use as ground truth for the layer feeding into the current layer.\n",
    "\n",
    "**Note** Unfortunately, the arbitrary intersection of Hamming balls is a topic that appears to only have been studied in a combinatoric sense. The most closest research in this field is actually with regards to \"binary means\" or the Closest String/Substring problems.\n",
    "\n",
    "Whilst the above pure mathematical representation is what should be ideally researched to provide the optimal solution to this issue - I do have a quick and dirty hack which does minimize collective distance to the ideal strings, and that is simple majority vote.\n",
    "\n",
    "### Binary Majority Vote\n",
    "\n",
    "Given a set of $n$ ideal input binary strings:\n",
    "\n",
    "```\n",
    "x_tilde_1 = +1 -1 +1 -1 +1 -1 +1 ... \n",
    "x_tilde_2 = +1 -1 -1 +1 -1 +1 -1 ...\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "x_tilde_n = -1 +1 +1 -1 +1 +1 +1 ...\n",
    "```\n",
    "\n",
    "We simply count the number of $+1$'s that appear in each entry and threshold at $n/2$:\n",
    "\n",
    "```\n",
    "x_tilde_count = 9 12 72 8 48 9 ...\n",
    "x_tilde_thres = -1 -1 +1 -1 +1 -1 ...\n",
    "```\n",
    "\n",
    "`x_tilde_thres` is taken to be ground truth $z$ for the above layer and we repeat the process up the chain to get deep learning. We repeat the perceptron boilerplate down below and then begin to implement out the majority vote scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 14:44:01.040822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 14:44:01.165122: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 14:44:03.291305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.309481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.309664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.310128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 14:44:03.310756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.310932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.311060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.714814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.715054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.715209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 14:44:03.715352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 418 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-11-15 14:44:03.715603: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# In this block, we repeat much of the boilerplate from the previous chapter\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "THRESHOLD = 128\n",
    "\n",
    "# Full data conversion\n",
    "x_train = ((x_train > THRESHOLD)*2 - 1).reshape(x_train.shape[0],28*28)\n",
    "x_test = ((x_test > THRESHOLD)*2 - 1).reshape(x_test.shape[0],28*28)\n",
    "y_train = tf.one_hot(y_train,depth=10)*2 - 1\n",
    "y_test = tf.one_hot(y_test,depth=10)*2 - 1\n",
    "\n",
    "\n",
    "# Set seed for reproducability\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the requisite thresholds\n",
    "W_THRES = 2\n",
    "b_THRES = 2\n",
    "\n",
    "# Counters and counter logic\n",
    "counter = 0\n",
    "counter_RESET = 10\n",
    "\n",
    "# Feedforward one sample\n",
    "def forward(W,b,x):\n",
    "    out = np.sign(x@W-b)\n",
    "    out[out == 0] = 1\n",
    "    return out\n",
    "\n",
    "# Discover which columns/bias terms are to blame\n",
    "def blame_columns(z,zhat,b_blame,bias,param):\n",
    "\n",
    "    # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    # Increment bias blame for false positives (too big!)\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            b_blame[idx] += 1\n",
    "\n",
    "    # Decrement bias blame for false negatives (too small!)\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            b_blame[idx] -= 1\n",
    "\n",
    "    # If bias threshold is crossed, reset blame and increment/decrement bias\n",
    "    for idx, i in enumerate(np.abs(b_blame)>param):\n",
    "        if np.sign(b_blame[idx]) > 0 and i:\n",
    "            bias[idx] += 1\n",
    "        elif np.sign(b_blame[idx]) < 0 and i:\n",
    "            bias[idx] -= 1\n",
    "\n",
    "    return false_pos, false_neg, b_blame, bias\n",
    "\n",
    "def blame_weights(x,false_pos,false_neg,W_blame,Weight,param):\n",
    "\n",
    "    # Binarize inputs\n",
    "    x = x > 0\n",
    "\n",
    "    # If a weight is found to be blame for a false positive attribute blame\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_not(np.logical_xor(Weight[:,idx]>0,x))):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 1\n",
    "\n",
    "    # If a weight is found to be blame for a false negative attribute blame\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_xor(Weight[:,idx]>0,x)):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 1\n",
    "\n",
    "    # Find where weights exceed the blame threshold\n",
    "    rows,cols = np.where(W_blame >= param)\n",
    "\n",
    "    # Reset blame counter and flip corresponding weight\n",
    "    for i,j in zip(rows,cols):\n",
    "        W_blame[i,j] = 0\n",
    "        Weight[i,j] = Weight[i,j] * -1\n",
    "    \n",
    "    return W_blame, Weight\n",
    "\n",
    "# Not much changes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the majority weight ideal input construction routine to build out the deep learning system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(z,W,fp,fn):\n",
    "\n",
    "    out = np.zeros(W.shape[0])\n",
    "\n",
    "    if not fp.any() and not fn.any():\n",
    "        return z\n",
    "\n",
    "    for idx, i in enumerate(fp):\n",
    "        if i:\n",
    "            out -= W[:,idx]\n",
    "    \n",
    "    for idx, i in enumerate(fn):\n",
    "        if i:\n",
    "            out += W[:,idx]\n",
    "            \n",
    "    return np.sign(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the binarized multi-layer perceptron with its caches and begin training on MNIST using this procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 248/60000 [00:06<24:24, 40.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m# Train Layer 1\u001b[39;00m\n\u001b[1;32m     43\u001b[0m fp, fn, b1_blame, b1 \u001b[39m=\u001b[39m blame_columns(y2,z1,b1_blame,b1,\u001b[39m16\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m W1_blame, W1 \u001b[39m=\u001b[39m blame_weights(x,fp,fn,W1_blame,W1,\u001b[39m256\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Forgiveness counter\u001b[39;00m\n\u001b[1;32m     47\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mlogical_and(z2 \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, y \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn [1], line 75\u001b[0m, in \u001b[0;36mblame_weights\u001b[0;34m(x, false_pos, false_neg, W_blame, Weight, param)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m i:\n\u001b[1;32m     74\u001b[0m         \u001b[39mfor\u001b[39;00m jdx, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39mlogical_not(np\u001b[39m.\u001b[39mlogical_xor(Weight[:,idx]\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m,x))):\n\u001b[0;32m---> 75\u001b[0m             \u001b[39mif\u001b[39;00m j:\n\u001b[1;32m     76\u001b[0m                 W_blame[jdx,idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[39m# If a weight is found to be blame for a false negative attribute blame\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = (np.random.uniform(0,1,(28*28,256)) > 0.5) * 2 - 1\n",
    "W2 = (np.random.uniform(0,1,(256,10)) > 0.5) * 2 - 1\n",
    "b1 = np.zeros(256)\n",
    "b2 = np.zeros(10)\n",
    "\n",
    "# Initialize blame counters\n",
    "W1_blame = np.zeros((28*28,256))\n",
    "W2_blame = np.zeros((256,10))\n",
    "b1_blame = np.zeros(256)\n",
    "b2_blame = np.zeros(10)\n",
    "\n",
    "# Counter Parameters\n",
    "counter = 0\n",
    "acc = 0\n",
    "acc_count = 0\n",
    "REPORT = 10000\n",
    "epochs = 3\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH \"+str(e+1))\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    for i in tqdm(indices):\n",
    "\n",
    "        x = x_train[i]\n",
    "        y = y_train[i]\n",
    "\n",
    "        # Forward pass\n",
    "        z1 = forward(W1,b1,x)\n",
    "        z2 = forward(W2,b2,z1)\n",
    "\n",
    "        # Backward pass, Layer 2\n",
    "        fp, fn, b2_blame, b2 = blame_columns(y,z2,b2_blame,b2,4)\n",
    "        W2_blame, W2 = blame_weights(z1,fp,fn,W2_blame,W2,16)\n",
    "\n",
    "        # Majority vote inter-layer glue\n",
    "        y2 = majority_vote(z1,W2,fp,fn)\n",
    "\n",
    "        # Train Layer 1\n",
    "        fp, fn, b1_blame, b1 = blame_columns(y2,z1,b1_blame,b1,16)\n",
    "        W1_blame, W1 = blame_weights(x,fp,fn,W1_blame,W1,256)\n",
    "\n",
    "        # Forgiveness counter\n",
    "        counter += 1 - np.sum(np.logical_and(z2 > 0, y > 0))\n",
    "        if counter >= counter_RESET:\n",
    "            W1_blame -= 1\n",
    "            W1_blame[W1_blame < 0] = 0\n",
    "            b1_blame = np.sign(b1_blame) * (np.abs(b1_blame) - 1)\n",
    "            W2_blame -= 1\n",
    "            W2_blame[W2_blame < 0 ] = 0\n",
    "            b2_blame = np.sign(b2_blame) * (np.abs(b2_blame) - 1)\n",
    "            counter = 0\n",
    "\n",
    "        # Accuracy metric counter\n",
    "        acc += np.sum(np.logical_and(z2 > 0, y > 0))\n",
    "        acc_count += 1\n",
    "        if acc_count >= REPORT:\n",
    "            print(\"Current Accuracy: \" + str(acc / REPORT))\n",
    "            acc_count = 0\n",
    "            acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'd like to emphasize that this improves performance significantly over a single layer perceptron - a good indicator that this method works. Whilst this particular approach offers less intuitive prettyness to understand the feature extraction performed here, we can still verify that this functions as expected from a generalization standpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (784,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m z2 \u001b[39m=\u001b[39m forward(W2,b2,z1)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Accuracy metric counter\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39;49mlogical_and(z2 \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m, y \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m))\n\u001b[1;32m      9\u001b[0m acc_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m acc_count \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m REPORT:\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (784,) "
     ]
    }
   ],
   "source": [
    "for x,y in zip(x_test,x_test):\n",
    "\n",
    "    # Forward pass\n",
    "    z1 = forward(W1,b1,x)\n",
    "    z2 = forward(W2,b2,z1)\n",
    "\n",
    "    # Accuracy metric counter\n",
    "    acc += np.sum(np.logical_and(z2 > 0, y > 0))\n",
    "    acc_count += 1\n",
    "    if acc_count >= REPORT:\n",
    "        print(\"Current Accuracy: \" + str(acc / REPORT))\n",
    "        acc_count = 0\n",
    "        acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([[1,2,3],[4,5,6]]),axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
