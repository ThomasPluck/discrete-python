{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow Discrete Optimizer Implementation\n",
        "\n",
        "Following a phone call with Koen, he recommended I take another look at the [quantizers.py](https://github.com/larq/larq/blob/main/larq/quantizers.py) file within `Larq`. This contains useful examples of using `tf.custom_gradient` decorators to create discrete gradients that realise the discrete optimizer in Tensorflow.\n",
        "\n",
        "First, we begin by defining the activation and its gradient."
      ],
      "metadata": {
        "id": "vneEt3bdsJud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "N0cPSYLemJZp"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Batch_size 1 implementation to agree with theory\n",
        "@tf.custom_gradient\n",
        "def linear(W,x,b):\n",
        "  \"\"\"Assumption here is that we do XOR accumulate,\n",
        "  although we get similar results we assume XNOR\"\"\"\n",
        "  bx = tf.broadcast_to(tf.transpose(x),W.shape)\n",
        "  e = ((1-bx) * W) + (bx * (1-W))\n",
        "\n",
        "  def grad(dy):\n",
        "    return dy * ((dy-1)//2 + e),\\\n",
        "           tf.cast(tf.math.reduce_sum(dy * ((dy-1)//2 + e),1) < e.shape[1]/2,tf.float32),\\\n",
        "           dy\n",
        "\n",
        "  return tf.cast(tf.math.reduce_sum(e,0) < b, tf.float32), grad\n",
        "\n",
        "# Arbitrary batch_size to agree with GPU reality\n",
        "@tf.custom_gradient\n",
        "def linear_batch(W,X,b):\n",
        "  \"\"\"Same as above except working on the assumption of batched data\"\"\"\n",
        "  batch_size = X.shape[0]\n",
        "  input_dims = X.shape[1]\n",
        "  output_dims = W.shape[1]\n",
        "\n",
        "  print(X,W)\n",
        "\n",
        "  bX = tf.broadcast_to(X,(output_dims,batch_size,input_dims))\n",
        "  bX = tf.transpose(bX,[1,2,0])\n",
        "  bW = tf.broadcast_to(W,(batch_size,input_dims,output_dims))\n",
        "\n",
        "  e = ((1-bX) * bW) + (bX * (1 - bW))\n",
        "\n",
        "  def grad(dy):\n",
        "    return tf.math.reduce_sum(dy * ((dy-1)//2 + e),0),\\\n",
        "            tf.cast(tf.math.reduce_sum(dy * ((dy-1)//2 + e),2) < e.shape[2]/2,tf.float32),\\\n",
        "            dy\n",
        "\n",
        "  return tf.cast(tf.math.reduce_sum(e,1),tf.float32)\n",
        "\n",
        "# False Positive: was it active when it shouldn't have been\n",
        "# False Negative: was it inactive when it shouldn't have been\n",
        "@tf.custom_gradient\n",
        "def loss(yhat,y):\n",
        "  e = (yhat * (1-y)) - ((1-yhat) * y)\n",
        "  def grad(dy):\n",
        "    dz_dyhat = e\n",
        "    dz_dy = e\n",
        "    return dy * dz_dyhat, dy * dz_dy\n",
        "\n",
        "  return e, grad\n",
        "\n",
        "# One hot encode label m out of n choices\n",
        "def one_hot(n,m):\n",
        "  out = np.zeros((1,n))\n",
        "  out[0,m] = 1\n",
        "  return tf.convert_to_tensor(out,tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we load and format MNIST for use in Tensorflow and train our 1 layer test model on it.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lj6yt1PbtJgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initial Data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = tf.reshape(tf.cast(x_train > 50, tf.float32),(60000,784))\n",
        "y_train = tf.convert_to_tensor(to_categorical(y_train))\n",
        "\n",
        "# Initial model parameters\n",
        "W = tf.cast(tf.random.uniform((28*28,10)) > 0.5,tf.float32)\n",
        "b = tf.zeros((1,10)) + 28*28/2\n",
        "weight_counters = np.zeros(W.shape)\n",
        "bias_counters = np.zeros(b.shape)\n",
        "\n",
        "# Important hyperparamaters\n",
        "WEIGHT_THRESHOLD = 4\n",
        "BIAS_THRESHOLD = 4\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Flip weights using counter/inertia/confidence information\n",
        "def flip_weights(counters, weights):\n",
        "  out1 = tf.where(counters > WEIGHT_THRESHOLD, 1-weights, weights)\n",
        "  out2 = tf.where(counters > WEIGHT_THRESHOLD, 0, counters)\n",
        "  return out1, out2\n",
        "\n",
        "# Increment/decrement biases with counter/interia/confidence information\n",
        "def indecr_biases(counters, biases):\n",
        "  out1 = tf.where(counters > BIAS_THRESHOLD, biases - tf.math.sign(counters), biases)\n",
        "  out2 = tf.where(counters > BIAS_THRESHOLD, 0, counters)\n",
        "  return out1, out2\n",
        "\n",
        "# Take step of size 1 towards 0 for all elements in a counter tensor\n",
        "def decr_counter(counter):\n",
        "  return tf.math.sign(counter) * (tf.math.sign(counter) * counter - 1)\n",
        "\n",
        "losses = []\n",
        "loss_cache = 0\n",
        "\n",
        "for i in tqdm(range(0, 60000, BATCH_SIZE)):\n",
        "\n",
        "  # 1 batch feed forward step\n",
        "  with tf.GradientTape(persistent = True) as tape:\n",
        "    X = x_train[i:i+BATCH_SIZE]\n",
        "    Y = y_train[i:i+BATCH_SIZE]\n",
        "    tape.watch([X,W,b,Y])\n",
        "    Yhat = linear_batch(W,X,b)\n",
        "    truth = loss(Yhat, Y)\n",
        "\n",
        "  # Compute and cache losses\n",
        "  loss_cache += tf.math.reduce_sum(abs(truth))\n",
        "\n",
        "  # Increment counters using gradient data\n",
        "  weight_counters += tape.gradient(truth,W)\n",
        "  bias_counters += tape.gradient(truth,b)\n",
        "\n",
        "  # Adjust weights and biases accordingly\n",
        "  W, weight_counters = flip_weights(weight_counters, W)\n",
        "  b, bias_counters = indecr_biases(bias_counters, b)\n",
        "\n",
        "  # Decrement counters\n",
        "  weight_counters = decr_counter(weight_counters)\n",
        "  bias_counters = decr_counter(bias_counters)\n",
        "\n",
        "  # Keep track of loss data\n",
        "  if i % (BATCH_SIZE * (1000 // BATCH_SIZE)) == 0:\n",
        "    losses.append(loss_cache/(BATCH_SIZE * (1000 // BATCH_SIZE)))\n",
        "    loss_cache = 0\n",
        "\n",
        "# Plot losses\n",
        "plt.plot(losses[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "k0nGbQAatNiS",
        "outputId": "2be875cc-5152-4a9e-b3e9-6a3468595f12"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3750 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(16, 784), dtype=float32) tf.Tensor(\n",
            "[[0. 1. 0. ... 0. 1. 0.]\n",
            " [0. 0. 1. ... 0. 0. 1.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 1. 1. ... 0. 1. 1.]\n",
            " [1. 0. 0. ... 0. 0. 1.]\n",
            " [1. 1. 1. ... 0. 0. 0.]], shape=(784, 10), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-d4730e0d1c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mYhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;34m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_eager_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m   \u001b[0;34m\"\"\"Implement custom gradient decorator for eager mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtape_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableWatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvariable_watcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0mall_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Are my biases training sensibly?\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96awMgZqkqz2",
        "outputId": "41fcaca5-74a5-4811-e342-3c8c4c182cb1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[162. 112. 142. 140. 136. 118. 141. 123. 119. 127.]], shape=(1, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3,3,figsize=(9,9))\n",
        "\n",
        "# Do the weights look like a statistical representation of MNIST classes?\n",
        "for i in range(9):\n",
        "  axs[i//3][i%3].imshow(tf.reshape(W[:,i+1],(28,28)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "H1gOWTo3_kkO",
        "outputId": "9e143986-8214-4915-8b7c-ad8b4dbb3f83"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tf.reshape(tf.cast(x_test > 50, tf.float32),(10000,1,784))\n",
        "y_test = tf.convert_to_tensor(to_categorical(y_test))\n",
        "\n",
        "tf.reduce_sum(tf.cast((x_test @ W) < b, tf.float32) == y_test)/10000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iLT-EZjE4nx",
        "outputId": "6e5984cf-e152-4b60-d34a-a8b4d30b8d6e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10000, 1, 10), dtype=float32, numpy=\n",
              "array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer0_neurons = 28*28\n",
        "layer1_neurons = 1024\n",
        "layer2_neurons = 1024\n",
        "layer3_neurons = 1024\n",
        "layer4_neurons = 1024\n",
        "layer5_neurons = 1024\n",
        "layer6_neurons = 1024\n",
        "layer7_neurons = 1024\n",
        "layer8_neurons = 1024\n",
        "layer9_neurons = 1024\n",
        "layer10_neurons = 10\n",
        "\n",
        "W1 = tf.cast(tf.random.uniform((layer0_neurons,layer1_neurons)) > 0.5,tf.float32)\n",
        "W2 = tf.cast(tf.random.uniform((layer1_neurons,layer2_neurons)) > 0.5,tf.float32)\n",
        "W3 = tf.cast(tf.random.uniform((layer2_neurons,layer3_neurons)) > 0.5,tf.float32)\n",
        "W4 = tf.cast(tf.random.uniform((layer3_neurons,layer4_neurons)) > 0.5,tf.float32)\n",
        "W5 = tf.cast(tf.random.uniform((layer4_neurons,layer5_neurons)) > 0.5,tf.float32)\n",
        "W6 = tf.cast(tf.random.uniform((layer5_neurons,layer6_neurons)) > 0.5,tf.float32)\n",
        "W7 = tf.cast(tf.random.uniform((layer6_neurons,layer7_neurons)) > 0.5,tf.float32)\n",
        "W8 = tf.cast(tf.random.uniform((layer7_neurons,layer8_neurons)) > 0.5,tf.float32)\n",
        "W9 = tf.cast(tf.random.uniform((layer8_neurons,layer9_neurons)) > 0.5,tf.float32)\n",
        "W10 = tf.cast(tf.random.uniform((layer9_neurons,layer10_neurons)) > 0.5,tf.float32)\n",
        "\n",
        "b1 = tf.zeros((1,layer1_neurons)) + layer1_neurons / 2\n",
        "b2 = tf.zeros((1,layer2_neurons)) + layer2_neurons / 2\n",
        "b3 = tf.zeros((1,layer3_neurons)) + layer3_neurons / 2\n",
        "b4 = tf.zeros((1,layer4_neurons)) + layer4_neurons / 2\n",
        "b5 = tf.zeros((1,layer5_neurons)) + layer5_neurons / 2\n",
        "b6 = tf.zeros((1,layer6_neurons)) + layer6_neurons / 2\n",
        "b7 = tf.zeros((1,layer7_neurons)) + layer7_neurons / 2\n",
        "b8 = tf.zeros((1,layer8_neurons)) + layer8_neurons / 2\n",
        "b9 = tf.zeros((1,layer9_neurons)) + layer9_neurons / 2\n",
        "b10 = tf.zeros((1,layer10_neurons)) + layer10_neurons / 2\n",
        "\n",
        "W1Cs = tf.zeros((layer0_neurons,layer1_neurons))\n",
        "W2Cs = tf.zeros((layer1_neurons,layer2_neurons))\n",
        "W3Cs = tf.zeros((layer2_neurons,layer3_neurons))\n",
        "W4Cs = tf.zeros((layer3_neurons,layer4_neurons))\n",
        "W5Cs = tf.zeros((layer4_neurons,layer5_neurons))\n",
        "W6Cs = tf.zeros((layer5_neurons,layer6_neurons))\n",
        "W7Cs = tf.zeros((layer6_neurons,layer7_neurons))\n",
        "W8Cs = tf.zeros((layer7_neurons,layer8_neurons))\n",
        "W9Cs = tf.zeros((layer8_neurons,layer9_neurons))\n",
        "W10Cs = tf.zeros((layer9_neurons,layer10_neurons))\n",
        "\n",
        "b1Cs = tf.zeros(layer1_neurons)\n",
        "b2Cs = tf.zeros(layer2_neurons)\n",
        "b3Cs = tf.zeros(layer3_neurons)\n",
        "b4Cs = tf.zeros(layer4_neurons)\n",
        "b5Cs = tf.zeros(layer5_neurons)\n",
        "b6Cs = tf.zeros(layer6_neurons)\n",
        "b7Cs = tf.zeros(layer7_neurons)\n",
        "b8Cs = tf.zeros(layer8_neurons)\n",
        "b9Cs = tf.zeros(layer9_neurons)\n",
        "b10Cs = tf.zeros(layer10_neurons)\n",
        "\n",
        "losses = []\n",
        "loss_cache = 0\n",
        "\n",
        "for i in tqdm(range(60000)):\n",
        "\n",
        "  with tf.GradientTape(persistent = True) as tape:\n",
        "    x = x_train[i]\n",
        "    y = y_train[i]\n",
        "    tape.watch([x,W1,W2,W3,W4,W5,W6,W7,W8,W9,W10,\n",
        "                b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,y])\n",
        "    z1 = linear(W1,x,b1)\n",
        "    z2 = linear(W2,z1,b2)\n",
        "    z3 = linear(W3,z2,b3)\n",
        "    z4 = linear(W4,z3,b4)\n",
        "    z5 = linear(W5,z4,b5)\n",
        "    z6 = linear(W6,z5,b6)\n",
        "    z7 = linear(W7,z6,b7)\n",
        "    z8 = linear(W8,z7,b8)\n",
        "    z9 = linear(W9,z8,b9)\n",
        "    yhat = linear(W10,z9,b10)\n",
        "\n",
        "    truth = loss(yhat,y)\n",
        "\n",
        "  loss_cache += tf.math.reduce_sum(abs(truth))\n",
        "\n",
        "  W1Cs += tape.gradient(truth,W1)\n",
        "  W2Cs += tape.gradient(truth,W2)\n",
        "  W3Cs += tape.gradient(truth,W3)\n",
        "  W4Cs += tape.gradient(truth,W4)\n",
        "  W5Cs += tape.gradient(truth,W5)\n",
        "  W6Cs += tape.gradient(truth,W6)\n",
        "  W7Cs += tape.gradient(truth,W7)\n",
        "  W8Cs += tape.gradient(truth,W8)\n",
        "  W9Cs += tape.gradient(truth,W9)\n",
        "  W10Cs += tape.gradient(truth,W10)\n",
        "\n",
        "  b1Cs += tape.gradient(truth,b1)\n",
        "  b2Cs += tape.gradient(truth,b2)\n",
        "  b3Cs += tape.gradient(truth,b3)\n",
        "  b4Cs += tape.gradient(truth,b4)\n",
        "  b5Cs += tape.gradient(truth,b5)\n",
        "  b6Cs += tape.gradient(truth,b6)\n",
        "  b7Cs += tape.gradient(truth,b7)\n",
        "  b8Cs += tape.gradient(truth,b8)\n",
        "  b9Cs += tape.gradient(truth,b9)\n",
        "  b10Cs += tape.gradient(truth,b10)\n",
        "\n",
        "  W1, W1Cs = flip_weights(W1Cs, W1)\n",
        "  W2, W2Cs = flip_weights(W2Cs, W2)\n",
        "  W3, W3Cs = flip_weights(W3Cs, W3)\n",
        "  W4, W4Cs = flip_weights(W4Cs, W4)\n",
        "  W5, W5Cs = flip_weights(W5Cs, W5)\n",
        "  W6, W6Cs = flip_weights(W6Cs, W6)\n",
        "  W7, W7Cs = flip_weights(W7Cs, W7)\n",
        "  W8, W8Cs = flip_weights(W8Cs, W8)\n",
        "  W9, W9Cs = flip_weights(W9Cs, W9)\n",
        "  W10, W10Cs = flip_weights(W10Cs, W10)\n",
        "\n",
        "  b1, b1Cs = indecr_biases(b1Cs, b1)\n",
        "  b2, b2Cs = indecr_biases(b2Cs, b2)\n",
        "  b3, b3Cs = indecr_biases(b3Cs, b3)\n",
        "  b4, b4Cs = indecr_biases(b4Cs, b4)\n",
        "  b5, b5Cs = indecr_biases(b5Cs, b5)\n",
        "  b6, b6Cs = indecr_biases(b6Cs, b6)\n",
        "  b7, b7Cs = indecr_biases(b7Cs, b7)\n",
        "  b8, b8Cs = indecr_biases(b8Cs, b8)\n",
        "  b9, b9Cs = indecr_biases(b9Cs, b9)\n",
        "  b10, b10Cs = indecr_biases(b10Cs, b10)\n",
        "\n",
        "  if i % 16 == 0 :\n",
        "\n",
        "    W1Cs = decr_counter(W1Cs)\n",
        "    W2Cs = decr_counter(W2Cs)\n",
        "    W3Cs = decr_counter(W3Cs)\n",
        "    W4Cs = decr_counter(W4Cs)\n",
        "    W5Cs = decr_counter(W5Cs)\n",
        "    W6Cs = decr_counter(W6Cs)\n",
        "    W7Cs = decr_counter(W7Cs)\n",
        "    W8Cs = decr_counter(W8Cs)\n",
        "    W9Cs = decr_counter(W9Cs)\n",
        "    W10Cs = decr_counter(W10Cs)\n",
        "\n",
        "    b1Cs = decr_counter(b1Cs)\n",
        "    b2Cs = decr_counter(b2Cs)\n",
        "    b3Cs = decr_counter(b3Cs)\n",
        "    b4Cs = decr_counter(b4Cs)\n",
        "    b5Cs = decr_counter(b5Cs)\n",
        "    b6Cs = decr_counter(b6Cs)\n",
        "    b7Cs = decr_counter(b7Cs)\n",
        "    b8Cs = decr_counter(b8Cs)\n",
        "    b9Cs = decr_counter(b9Cs)\n",
        "    b10Cs = decr_counter(b10Cs)\n",
        "\n",
        "  if i % 1000 == 0:\n",
        "\n",
        "    losses.append(loss_cache/1000)\n",
        "    loss_cache = 0\n",
        "\n",
        "plt.plot(losses[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "pbZA5JcJ3_9R",
        "outputId": "4625950a-2fea-46bc-dd3e-5ed104cc70e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 10736/60000 [20:35<1:34:28,  8.69it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8766b1575af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 b1,b2,b3,b4,b5,b6,b7,b8,b9,b10,y])\n\u001b[1;32m     67\u001b[0m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mz3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mz4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;34m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_eager_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m   \u001b[0;34m\"\"\"Implement custom gradient decorator for eager mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtape_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableWatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvariable_watcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0mall_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f3fbb1a213e8>\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(W, x, b)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"Assumption here is that we do XOR accumulate,\n\u001b[1;32m      8\u001b[0m   although we get similar results we assume XNOR\"\"\"\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mbx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[0;34m(input, shape, name)\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m       return broadcast_to_eager_fallback(\n\u001b[0m\u001b[1;32m    859\u001b[0m           input, shape, name=name, ctx=_ctx)\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mbroadcast_to_eager_fallback\u001b[0;34m(input, shape, name, ctx)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast_to_eager_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m   \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tidx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;31m# not list allowed dtypes, in which case we should skip this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallowed_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;31m# If we did not match an allowed dtype, try again with the default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# dtype. This could be because we have an empty tensor and thus we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[0;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    359\u001b[0m     raise ValueError(\n\u001b[1;32m    360\u001b[0m         f\"Cannot convert a partially known TensorShape {s} to a Tensor.\")\n\u001b[0;32m--> 361\u001b[0;31m   \u001b[0ms_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m   \u001b[0mint64_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape %s is not fully defined\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m     \"\"\"Returns a list of integers or `None` for each dimension.\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}