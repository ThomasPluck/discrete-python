{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "With the basic principles of multi-layer perceptrons understood, extending the same ideas to convolution relies on understanding that to a convolutional filter, a single sample can actually be multiple input samples that the discrete optimizer has to work with.\n",
    "\n",
    "In dealing with convolutions, we can consider each subsampled region to be a legitimate source of blame for any dodgy assessment made by the network itself. For absolute simplicity (and to save time running this) - we will assume there is a single 5-by-5 convolutional filter with stride of 1 moving across a 28-by-28 MNIST image.\n",
    "\n",
    "In this case, we expect 24 * 24 outputs which can then be fed into an ordinary perceptron to train.\n",
    "\n",
    "We'll begin by implementing the convolutional filter and its forward/backward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "\n",
    "# We'll assume that the input is correctly folded into 2D\n",
    "def conv_forward(filter,bias,x):\n",
    "    out = convolve2d(x,filter).flatten()\n",
    "    return np.sign(out - bias)\n",
    "\n",
    "def conv_blame(x, false_pos, false_neg, filter, bias, filter_blame, bias_blame, filter_thres, bias_thres):\n",
    "\n",
    "    for idx, i in enumerate(false_pos):\n",
    "\n",
    "        # A less lazy person would generalize this for all possible convolution\n",
    "        if i:\n",
    "            row = idx // 24\n",
    "            col = idx % 24\n",
    "            sample = x[row:row+5,col:col+5]\n",
    "            filter_blame += abs(sample + filter) // 2 # If contributed to FP, blame it!\n",
    "            bias_blame[idx] += 1 # increment bias blame\n",
    "\n",
    "    for idx, i in enumerate(false_neg):\n",
    "\n",
    "        if i:\n",
    "            row = idx // 24\n",
    "            col = idx % 24\n",
    "            sample = x[row:row+5,col:col+5]\n",
    "            filter_blame += 1 - (abs(sample + filter) // 2) # Opposite is true if contributed to FN\n",
    "            bias_blame[idx] -= 1 # decrement bias blame\n",
    "\n",
    "    # Flip weights where appropriate\n",
    "    filter_blame_bool = filter_blame >= filter_thres\n",
    "    if filter_blame_bool.any():\n",
    "\n",
    "        for i in range(np.where(filter_blame_bool)):\n",
    "            filter[i//5,i%5] *= -1\n",
    "            filter_blame[i//5,i%5] = 0\n",
    "\n",
    "    # Increment/decrement bias where appropriate\n",
    "    bias_blame = np.abs(bias_blame) >= bias_thres\n",
    "    if bias_blame.any():\n",
    "\n",
    "        for i in range(np.where(bias_blame)):\n",
    "\n",
    "            if np.abs(bias_blame[i]) > 0:\n",
    "                bias += 1\n",
    "            else:\n",
    "                bias -= 1\n",
    "\n",
    "            bias_blame[i] = 0\n",
    "\n",
    "    return filter, bias, filter_blame, bias_blame\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add all of the original boilerplate from the previous two chapters and write up the final network to examine the performance of our proposed convolutional neural network optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "THRESHOLD = 128\n",
    "\n",
    "# Full data conversion\n",
    "x_train = ((x_train > THRESHOLD)*2 - 1)\n",
    "x_test = ((x_test > THRESHOLD)*2 - 1)\n",
    "y_train = tf.one_hot(y_train,depth=10)*2 - 1\n",
    "y_test = tf.one_hot(y_test,depth=10)*2 - 1\n",
    "\n",
    "\n",
    "# Set seed for reproducability\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Feedforward one sample\n",
    "def forward(W,b,x):\n",
    "    return np.sign(x@W-b)\n",
    "\n",
    "def compute_fpfn(z,zhat):\n",
    "       # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    false_pos = false_pos * 2 - 1\n",
    "    false_neg = false_neg * 2 - 1\n",
    "\n",
    "    return false_pos, false_neg\n",
    "\n",
    "# Discover which columns/bias terms are to blame\n",
    "def blame_columns(z,zhat,b_blame,bias,param):\n",
    "\n",
    "    # Binarize inputs\n",
    "    z = z > 0\n",
    "    zhat = zhat > 0\n",
    "\n",
    "    # Compute where there are false positives and false negatives\n",
    "    false_pos = np.logical_and(zhat,np.logical_not(z))\n",
    "    false_neg = np.logical_and(np.logical_not(zhat),z)\n",
    "\n",
    "    # Increment bias blame for false positives (too big!)\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            b_blame[idx] += 1\n",
    "\n",
    "    # Decrement bias blame for false negatives (too small!)\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            b_blame[idx] -= 1\n",
    "\n",
    "    # If bias threshold is crossed, reset blame and increment/decrement bias\n",
    "    for idx, i in enumerate(np.abs(b_blame)>param):\n",
    "        if np.sign(b_blame[idx]) > 0 and i:\n",
    "            bias[idx] += 1\n",
    "        elif np.sign(b_blame[idx]) < 0 and i:\n",
    "            bias[idx] -= 1\n",
    "\n",
    "    return false_pos, false_neg, b_blame, bias\n",
    "\n",
    "def blame_weights(x,false_pos,false_neg,W_blame,Weight,param):\n",
    "\n",
    "    # Binarize inputs\n",
    "    x = x > 0\n",
    "\n",
    "    # If a weight is found to be blame for a false positive attribute blame\n",
    "    for idx, i in enumerate(false_pos):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_not(np.logical_xor(Weight[:,idx]>0,x))):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 1\n",
    "\n",
    "    # If a weight is found to be blame for a false negative attribute blame\n",
    "    for idx, i in enumerate(false_neg):\n",
    "        if i:\n",
    "            for jdx, j in enumerate(np.logical_xor(Weight[:,idx]>0,x)):\n",
    "                if j:\n",
    "                    W_blame[jdx,idx] += 1\n",
    "\n",
    "    # Find where weights exceed the blame threshold\n",
    "    rows,cols = np.where(W_blame >= param)\n",
    "\n",
    "    # Reset blame counter and flip corresponding weight\n",
    "    for i,j in zip(rows,cols):\n",
    "        W_blame[i,j] = 0\n",
    "        Weight[i,j] = Weight[i,j] * -1\n",
    "    \n",
    "    return W_blame, Weight\n",
    "\n",
    "# Majority vote inter-layer glue\n",
    "def majority_vote(z,W,fp,fn):\n",
    "\n",
    "    out = np.zeros(W.shape[0])\n",
    "\n",
    "    if not fp.any() and not fn.any():\n",
    "        return z\n",
    "\n",
    "    for idx, i in enumerate(fp):\n",
    "        if i:\n",
    "            out -= W[:,idx]\n",
    "    \n",
    "    for idx, i in enumerate(fn):\n",
    "        if i:\n",
    "            out += W[:,idx]\n",
    "            \n",
    "    return np.sign(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define and train this CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Convolutional filter and bias\n",
    "conv_filter = (np.random.uniform(0,1,(5,5)) < 0.5) * 2 - 1\n",
    "conv_bias = np.random.randint(0,25,24*24)\n",
    "\n",
    "# Linear weights and biases\n",
    "linear_weight = (np.random.uniform(0,1,(24*24,10)) < 0.5) * 2 - 1\n",
    "linear_bias = np.random.randint(-144,144,10)\n",
    "\n",
    "# Blame counters\n",
    "filter_blame = np.zeros((5,5))\n",
    "conv_bias_blame = np.zeros(24*24)\n",
    "weight_blame = np.zeros((24*24,10))\n",
    "lin_bias_blame = np.zeros(10)\n",
    "\n",
    "# Counters\n",
    "epochs = 3\n",
    "counter = 0\n",
    "acc_count = 0\n",
    "COUNT_RESET = 10\n",
    "ACC_RESET = 10000\n",
    "\n",
    "# Iterate over epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(\"EPOCH \"+str(e+1))\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Iterate over samples\n",
    "    for i in tqdm(indices):\n",
    "        \n",
    "        # Load training samples\n",
    "        x = x_train[i]\n",
    "        y = y_train[i]\n",
    "\n",
    "        # Forward pass\n",
    "        z1 = conv_forward(conv_filter, conv_bias, x)\n",
    "        z2 = forward(linear_weight, linear_bias, z1)\n",
    "\n",
    "        # Backward pass on perceptron layer\n",
    "        fp,fn,lin_bias_blame,linear_bias = blame_columns(z2,y,lin_bias_blame,linear_bias,4)\n",
    "        weight_blame, linear_weight = blame_weights(z1,fp,fn,weight_blame,linear_weight,4)\n",
    "\n",
    "        # Compute inter-layer majority vote\n",
    "        y2 = majority_vote(y,linear_weight,fp,fn)\n",
    "\n",
    "        # Backward pass on convolutional layer\n",
    "        fp, fn = compute_fpfn(y2,z1)\n",
    "        conv_filter, conv_bias, filter_blame, conv_bias_blame = conv_blame(x,fp,fn,conv_filter,\n",
    "                                                                           conv_bias,filter_blame,\n",
    "                                                                           conv_bias_blame,4,4)\n",
    "\n",
    "        # Forgiveness counter\n",
    "        counter += 1 - np.sum(np.logical_and(z2 > 0, y > 0))\n",
    "        if counter >= COUNT_RESET:\n",
    "            filter_blame -= 1\n",
    "            filter_blame[filter_blame < 0] = 0\n",
    "            conv_bias_blame = np.sign(conv_bias_blame) * (np.abs(conv_bias_blame) - 1)\n",
    "            linear_weight -= 1\n",
    "            linear_weight[linear_weight < 0 ] = 0\n",
    "            lin_bias_blame = np.sign(lin_bias_blame) * (np.abs(lin_bias_blame) - 1)\n",
    "            counter = 0\n",
    "\n",
    "        # Accuracy metric counter\n",
    "        acc += np.sum(np.logical_and(z2 > 0, y > 0))\n",
    "        acc_count += 1\n",
    "        if acc_count >= ACC_RESET:\n",
    "            print(\"Current Accuracy: \" + str(acc / ACC_RESET))\n",
    "            acc_count = 0\n",
    "            acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let us test on an independent sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(\"EPOCH \"+str(e+1))\n",
    "    indices = np.arange(x_test.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Iterate over samples\n",
    "    for i in tqdm(indices):\n",
    "        \n",
    "        # Load training samples\n",
    "        x = x_test[i]\n",
    "        y = y_test[i]\n",
    "\n",
    "        # Forward pass\n",
    "        z1 = conv_forward(conv_filter, conv_bias, x)\n",
    "        z2 = forward(linear_weight, linear_bias, z1)\n",
    "\n",
    "        # Accuracy metric counter\n",
    "        acc += np.sum(np.logical_and(z2 > 0, y > 0))\n",
    "        acc_count += 1\n",
    "        if acc_count >= ACC_RESET:\n",
    "            print(\"Current Accuracy: \" + str(acc / ACC_RESET))\n",
    "            acc_count = 0\n",
    "            acc = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
